import torch
import torch.nn as nn
import torch.nn.functional as F

# ç”¨ä¾†åµæ¸¬ YOLOv5 çš„ Conv blockï¼ˆå¿…è¦ï¼‰
from models.common import Conv as YoloConv


# ============================================================
# Domain-Aware Adapter
#   - invertible branch (Finv)
#   - spatial branch (Fsp)
# ============================================================
class DomainAwareAdapter(nn.Module):
    def __init__(self, channels: int, rank_inv: int = 8, rank_sp: int = 8):
        """
        channels: Conv block è¼¸å‡ºçš„ channel æ•¸
        rank_inv: Finv bottleneck rank
        rank_sp: Fsp bottleneck rank
        """
        super().__init__()

        r_inv = max(1, rank_inv)
        r_sp = max(1, rank_sp)

        # ---- Invertible åˆ†æ”¯ Finv ----
        self.down_inv = nn.Conv2d(channels, r_inv, kernel_size=1, bias=False)
        self.up_inv   = nn.Conv2d(r_inv, channels, kernel_size=1, bias=False)

        # ---- Spatial åˆ†æ”¯ Fsp ----
        self.down_sp = nn.Conv2d(channels, r_sp, kernel_size=1, bias=False)
        self.up_sp   = nn.Conv2d(r_sp, channels, kernel_size=1, bias=False)

        # èåˆæ™‚çš„ scaleï¼ˆåˆå§‹åŒ–ç‚º 0ï¼‰
        self.scale = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        """
        x: feature map, [B, C, H, W]
        """
        # ------------------------------------------------
        # ğŸ›¡ï¸ æœ€å¾Œé˜²ç·šï¼šå¦‚æœ adapter æ¬Šé‡é‚„æ²’æ¬åˆ° GPUï¼Œå°±è‡ªå‹•è·Ÿä¸Š
        # ------------------------------------------------
        if self.down_inv.weight.device != x.device:
            self.to(x.device)

        # ---- invertible branch ----
        z_inv = F.relu(self.down_inv(x))
        y_inv = self.up_inv(z_inv)

        # ---- spatial branch ----
        z_sp = F.relu(self.down_sp(x))
        y_sp = self.up_sp(z_sp)

        # ---- adapter output ----
        return x + self.scale * (y_inv + y_sp)


# ============================================================
# æ›ä¸Š adaptersï¼ˆåªå° YOLOv5 Conv blockï¼‰
# ============================================================
def add_adapters_to_model(model: nn.Module, min_channels: int = 128, rank: int = 8):
    """
    åœ¨ YOLOv5 backbone/neck/head ä¸­æ‰€æœ‰ Conv block ä¸Šæ›ä¸Š DomainAwareAdapter
    è‹¥è©²å±¤ out_channels < min_channelsï¼Œå‰‡ç•¥éï¼ˆå¤ªå°æ²’å¿…è¦ï¼‰
    """
    adapters = []

    for m in model.modules():
        if isinstance(m, YoloConv):

            out_ch = m.conv.out_channels

            if out_ch >= min_channels:

                # é¿å…é‡è¤‡æ›
                if not hasattr(m, "adapter"):
                    m.adapter = DomainAwareAdapter(
                        channels=out_ch,
                        rank_inv=rank,
                        rank_sp=rank,
                    )
                    adapters.append(m.adapter)

    return adapters


# ============================================================
# åªè®“ adapter å¯ä»¥è¨“ç·´
# ============================================================
def set_adapter_trainable(model: nn.Module):
    """ Freeze YOLO backboneï¼Œå•Ÿå‹• adapter åƒæ•¸ """
    for p in model.parameters():
        p.requires_grad = False

    for m in model.modules():
        if isinstance(m, DomainAwareAdapter):
            for p in m.parameters():
                p.requires_grad = True


# ============================================================
# æ‹¿å‡ºæ‰€æœ‰ adaptersï¼ˆè¨“ç·´æ™‚ç”¨ï¼‰
# ============================================================
def get_all_adapters(model: nn.Module):
    return [m for m in model.modules() if isinstance(m, DomainAwareAdapter)]


# ============================================================
# Orthogonality Lossï¼ˆè®“ Finv èˆ‡ Fsp æ­£äº¤ï¼‰
# ============================================================
def adapter_orth_loss(adapters):
    """
    L_orth = mean(|| W_inv * W_sp^T ||^2)
    ç¢ºä¿å…©å€‹ä½ç§©å­ç©ºé–“åˆ†é›¢ï¼ˆé¿å… collapseï¼‰
    """
    loss = 0.0

    for ad in adapters:
        # å– bottleneck weights
        W_inv = ad.down_inv.weight.view(ad.down_inv.out_channels, -1)
        W_sp  = ad.down_sp.weight.view(ad.down_sp.out_channels, -1)

        # å…©è€…çš„ Gramï¼ˆäº¤å‰ç›¸é—œï¼‰
        G = W_inv @ W_sp.t()

        # Frobenius norm
        loss += (G.pow(2).mean())

    return loss / max(1, len(adapters))

